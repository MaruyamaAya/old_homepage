@inproceedings{liu2023hanayo,
author = {Liu, Ziming and Cheng, Shenggan and Zhou, Haotian and You, Yang},
title = {Hanayo: Harnessing Wave-like Pipeline Parallelism for Enhanced  Large Model Training Efficiency},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3607073},
doi = {10.1145/3581784.3607073},
abstract = {Large-scale language models have become increasingly challenging and expensive to train. Among various methods addressing this issue, Pipeline Parallelism has been widely employed to accommodate massive model weights within limited GPU memory. This paper introduces Hanayo, a wave-like pipeline parallelism strategy that boasts a concise structure and practical applicability, alongside a high-performance pipeline execution runtime to tackle the challenges of pipeline strategy implementation. Hanayo mitigates the issues of pipeline bubbles and excessive memory consumption prevalent in existing schemes, without resorting to model duplicates as in Chimera. Our evaluation, conducted on four distinct computing clusters and involving both GPT-like and BERT-like architectures with up to 32 GPUs, demonstrates up to a 30.4 \% increase in throughput compared to the state-of-the-art approach.},
booktitle = {SC '23, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {56},
numpages = {13},
keywords = {pipeline parallelism, high performance computing, distributed deep learning, large scale training},
location = {Denver, CO, USA},
html = {https://dl.acm.org/doi/10.1145/3581784.3607073},
series = {SC '23},
      selected = {true},
      preview = {hanayo.jpg},
}

@inproceedings{MLSYS2024_5431dca7,
 author = {Zhao, Xuanlei and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang},
 booktitle = {MLSys 2024, Proceedings of Machine Learning and Systems},
 editor = {P. Gibbons and G. Pekhimenko and C.M. De Sa},
 pages = {162--172},
 title = {HeteGen: Efficient Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices},
 url = {https://proceedings.mlsys.org/paper_files/paper/2024/file/5431dca75a8d2abc1fb51e89e8324f10-Paper-Conference.pdf},
 volume = {6},
 year = {2024},
 selected = {true},
 preview = {hetegen.png},
}


@misc{du2022energonai,
      title={EnergonAI: An Inference System for 10-100 Billion Parameter Transformer Models},
      author={Jiangsu Du and Ziming Liu and Jiarui Fang and Shenggui Li and Yongbin Li and Yutong Lu and Yang You},
      year={2022},
      eprint={2209.02341},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      selected={true},
      preview = {energon.jpg},
      arxiv = {https://arxiv.org/abs/2209.02341},
}
@misc{cheng2023atp,
      title={ATP: Adaptive Tensor Parallelism for Foundation Models},
      author={Shenggan Cheng and Ziming Liu and Jiangsu Du and Yang You},
      year={2023},
      eprint={2301.08658},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      selected = {true},
      preview = {atp.jpg},
      arxiv = {https://arxiv.org/abs/2301.08658},
}

@misc{zhao2024dsp,
      title={DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers},
      author={Xuanlei Zhao and Shenggan Cheng and Zangwei Zheng and Zheming Yang and Ziming Liu and Yang You},
      year={2024},
      eprint={2403.10266},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      selected = {true},
      preview = {dsp.png},
      arxiv = {https://arxiv.org/abs/2403.10266},
}

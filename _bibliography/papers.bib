@inproceedings{liu2023hanayo,
author = {Liu, Ziming and Cheng, Shenggan and Zhou, Haotian and You, Yang},
title = {Hanayo: Harnessing Wave-like Pipeline Parallelism for Enhanced  Large Model Training Efficiency},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3607073},
doi = {10.1145/3581784.3607073},
abstract = {Large-scale language models have become increasingly challenging and expensive to train. Among various methods addressing this issue, Pipeline Parallelism has been widely employed to accommodate massive model weights within limited GPU memory. This paper introduces Hanayo, a wave-like pipeline parallelism strategy that boasts a concise structure and practical applicability, alongside a high-performance pipeline execution runtime to tackle the challenges of pipeline strategy implementation. Hanayo mitigates the issues of pipeline bubbles and excessive memory consumption prevalent in existing schemes, without resorting to model duplicates as in Chimera. Our evaluation, conducted on four distinct computing clusters and involving both GPT-like and BERT-like architectures with up to 32 GPUs, demonstrates up to a 30.4 \% increase in throughput compared to the state-of-the-art approach.},
booktitle = {SC '23, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {56},
numpages = {13},
keywords = {pipeline parallelism, high performance computing, distributed deep learning, large scale training},
location = {Denver, CO, USA},
html = {https://dl.acm.org/doi/10.1145/3581784.3607073},
series = {SC '23},
      selected = {true},
      preview = {hanayo.jpg},
}
@misc{zhao2024hetegen,
      title={HeteGen: Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices},
      author={Xuanlei Zhao and Bin Jia and Haotian Zhou and Ziming Liu and Shenggan Cheng and Yang You},
      year={2024},
      booktitle = {MLSys 2024,Proceedings of the 7th MLSys Conference},
      selected = {true},
      primaryClass={cs.PF},
      preview = {hetegen.png},
      series = {MLSys 2024},
      arxiv = {https://arxiv.org/abs/2403.01164},
}

@misc{du2022energonai,
      title={EnergonAI: An Inference System for 10-100 Billion Parameter Transformer Models},
      author={Jiangsu Du and Ziming Liu and Jiarui Fang and Shenggui Li and Yongbin Li and Yutong Lu and Yang You},
      year={2022},
      eprint={2209.02341},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      selected={true},
      preview = {energon.jpg},
      arxiv = {https://arxiv.org/abs/2209.02341},
}
@misc{cheng2023atp,
      title={ATP: Adaptive Tensor Parallelism for Foundation Models},
      author={Shenggan Cheng and Ziming Liu and Jiangsu Du and Yang You},
      year={2023},
      eprint={2301.08658},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      selected = {true},
      preview = {atp.jpg},
      arxiv = {https://arxiv.org/abs/2301.08658},
}

@misc{zhao2024dsp,
      title={DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers},
      author={Xuanlei Zhao and Shenggan Cheng and Zangwei Zheng and Zheming Yang and Ziming Liu and Yang You},
      year={2024},
      eprint={2403.10266},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      selected = {true},
      preview = {dsp.png},
      arxiv = {https://arxiv.org/abs/2403.10266},
}
